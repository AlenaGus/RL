{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid:\n",
    "    def __init__(self, height, width):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        \n",
    "    def configure(self, rewards, actions, transitions, start):\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.start = start\n",
    "        self.restart()\n",
    "        \n",
    "    def restart(self, start=None, random=False):\n",
    "        if random: \n",
    "            K = list(self.actions.keys())\n",
    "            start = K[np.random.choice(len(K))]\n",
    "        elif start is None: \n",
    "            start = self.start\n",
    "        self.state = start\n",
    "        \n",
    "    def is_terminal_state(self, s):\n",
    "        return s not in self.actions\n",
    "    \n",
    "    def is_game_over(self):\n",
    "        return self.is_terminal_state(self.state)\n",
    "    \n",
    "    def all_states(self):\n",
    "        for r in range(self.height):\n",
    "            for c in range(self.width):\n",
    "                yield (r,c)\n",
    "    \n",
    "    def all_actions(self):\n",
    "        return list(set(s for a in std_grid.actions.values() for s in a))\n",
    "    \n",
    "    def move(self, a):\n",
    "        if self.is_game_over(): return 0.0\n",
    "        \n",
    "        # get the transition distribution from the current state with action a\n",
    "        targets = self.transitions(self.state, a)        \n",
    "        if not targets: return 0 # move is not possible\n",
    "        \n",
    "        print (targets)\n",
    "        K = list(targets.keys())\n",
    "        V = list(targets.values())\n",
    "        self.state = K[np.random.choice(len(K), p=V)]\n",
    "        \n",
    "        return self.rewards.get(self.state, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_grid():\n",
    "    g = Grid(5,5)\n",
    "    \n",
    "    rewards = { (4,4): 10.0, \n",
    "                (2,2): -10.0,\n",
    "                (4,1): 0.3\n",
    "              }\n",
    "    \n",
    "    actions = { (0,0): ['R', 'D'],\n",
    "                (0,1): ['L', 'D'],\n",
    "                (0,3): ['D', 'R'],\n",
    "                (0,4): ['D', 'L'],\n",
    "                (1,0): ['U', 'D','R'],\n",
    "                (1,1): ['U', 'D', 'R', 'L'],\n",
    "                (1,2): ['D', 'R', 'L'],\n",
    "                (1,3): ['U', 'D', 'R','L'],\n",
    "                (1,4): ['D', 'L', 'U'],\n",
    "                (2,0): ['U', 'D', 'R'],\n",
    "                (2,1): ['U', 'D','L','R'],\n",
    "                #(2,2): ['U', 'D', 'R', 'L'],\n",
    "                (2,3): ['U', 'D', 'R','L'],\n",
    "                (2,4): ['U', 'D','L'],\n",
    "                (3,0): ['U', 'D', 'R'],\n",
    "                (3,1): ['U', 'D', 'R','L'],\n",
    "                (3,2): ['U', 'D', 'R','L'],\n",
    "                (3,3): ['U', 'D', 'R','L'],\n",
    "                (3,4): ['U', 'D','L'],\n",
    "                (4,0): ['U', 'R'],\n",
    "                (4,1): ['U', 'R','L'],\n",
    "                (4,2): ['U', 'R','L'],\n",
    "                (4,3): ['U', 'R','L']\n",
    "              }\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        if s not in actions:return {}\n",
    "        if a not in actions[s]: return {}\n",
    "        if a=='D': return {(s[0]+1, s[1]  ): 1.0}\n",
    "        if a=='U': return {(s[0]-1, s[1]  ): 1.0}\n",
    "        if a=='R': return {(s[0]  , s[1]+1): 1.0}\n",
    "        if a=='L': return {(s[0]  , s[1]-1): 1.0}\n",
    "\n",
    "    \n",
    "    g.configure(rewards, actions, transitions, (0.0))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value(V, g):\n",
    "    print (\"Value function\")\n",
    "    for c in range(g.width):\n",
    "        print(\"+-------\", end='')\n",
    "    print(\"+\")\n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            print (f\"| {V.get((r,c),0):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------\", end='')\n",
    "        print(\"+\")\n",
    "        \n",
    "        \n",
    "def print_policy(pi, g):\n",
    "    print (\"Policy\")\n",
    "    \n",
    "    for c in range(g.width):\n",
    "        print(\"+-------------\", end='')\n",
    "    print(\"+\")\n",
    "    \n",
    "    for r in range(g.height):\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'U', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"| {pi(g, 'L', s):+0.2f} {pi(g, 'R', s):+0.2f} \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            s = (r,c)\n",
    "            print (f\"|    {pi(g, 'D', s):+0.2f}    \", end='')\n",
    "        print(\"|\")\n",
    "        for c in range(g.width):\n",
    "            print(\"+-------------\", end='')\n",
    "        print(\"+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(grid, pi, gamma, debug=False):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"--- Loop #{loop}\")\n",
    "        loop += 1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if debug: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            # Bellman equation : sum over all available actions\n",
    "            v = 0\n",
    "            for a in grid.actions[s]:\n",
    "                if debug: print (f\"      Action: {a}\")\n",
    "                \n",
    "                # get the probability of taking action 'a' while in state 's'\n",
    "                pa = pi(grid, a, s)\n",
    "                if debug: print (f\"          pa={pa}\")\n",
    "                if pa == 0: continue\n",
    "                \n",
    "                # get the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if debug: print (f\"          targets={targets}\")\n",
    "                if not targets: continue\n",
    "                \n",
    "                for next_state, prob in targets.items():\n",
    "                    if debug: print (f\"              next_state={next_state}, prob={prob}\")\n",
    "                    if debug: print (f\"              V[next_state]={V[next_state]}\")\n",
    "                    v += pa * prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            \n",
    "            delta = max(delta, abs(v - V[s]))\n",
    "            if debug: print (f\"   Vs={V[s]} -> {v} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = v\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "                \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.50 +0.00 | +0.00 +0.00 | +0.00 +0.50 | +0.50 +0.00 |\n",
      "|    +0.50    |    +0.50    |    +0.00    |    +0.50    |    +0.50    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.33 +0.33 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.33    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.00 +0.00 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.25    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.25 +0.25 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.25    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.33    |    +0.33    |    +0.33    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.33 +0.33 | +0.33 +0.33 | +0.33 +0.33 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -2.11 | -2.36 | +0.00 | -1.73 | -1.34 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -2.32 | -3.14 | -5.03 | -2.50 | -1.25 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -2.49 | -4.25 | +0.00 | -3.11 | -0.33 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -1.72 | -2.16 | -2.98 | +0.12 | +3.27 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -1.07 | -1.00 | -0.09 | +3.34 | +0.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def random_policy(grid, a, s):\n",
    "    if s not in grid.actions: return 0.0\n",
    "    if a not in grid.actions[s]: return 0.0\n",
    "    return 1.0 / len(grid.actions[s])\n",
    "\n",
    "std_grid = build_standard_grid()\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, random_policy, 0.9)\n",
    "\n",
    "print_policy(random_policy, std_grid)\n",
    "print_value (V, std_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +4.98 | +5.31 | +0.00 | +6.56 | +7.29 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.53 | +5.90 | +6.56 | +7.29 | +8.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +6.15 | -10.00 | +0.00 | +8.10 | +9.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +6.83 | +7.29 | +8.10 | +9.00 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +7.59 | +8.10 | +9.00 | +10.00 | +0.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def fixed_policy(grid, a, s):\n",
    "    fixed = { (0,0):'D', (1,0):'D', (2,0):'D', (3,0):'D',\n",
    "              (4,0):'R', (4,1):'R', (4,2):'R', (4,3):'R',\n",
    "              (0,4):'D', (1,4):'D', (2,4):'D', (3,4):'D',\n",
    "             (0,1):'D', (1,1):'R', (1,2):'R', (1,3): 'R',\n",
    "             (2,1):'R',(2,2):'R', (2,3):'R', (0,3) :'R',\n",
    "             (3,1):'R',(3,2):'R', (3,3):'R'\n",
    "            }\n",
    "    return 1.0 if s in fixed and fixed[s] == a else 0.0\n",
    "\n",
    "\n",
    "V = iterative_policy_evaluation(std_grid, fixed_policy, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, std_grid)\n",
    "print_value (V, std_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_fixed_policy(state_action_map):\n",
    "    return lambda grid, a, s: 1.0 if s in state_action_map and state_action_map[s] == a else 0.0\n",
    "\n",
    "def policy_iteration(grid, gamma, debug=False):\n",
    "    # randomly initialize the actions at each state\n",
    "    V = { s:np.random.random() for s in grid.all_states() }\n",
    "    A = { s:np.random.choice([a for a in grid.all_actions()]) for s in grid.all_states() }\n",
    "    \n",
    "    # create a policy function\n",
    "    pi = build_fixed_policy(A)\n",
    "\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"Loop #{loop}: A={A}\")\n",
    "        loop+=1\n",
    "        \n",
    "        # policy evaluation\n",
    "        V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "        if debug: print (f\"         V={V}\")\n",
    "        \n",
    "        # policy improvement\n",
    "        policy_changed = False\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"    State {s}\")\n",
    "            \n",
    "            # init max with current values\n",
    "            maxV = float('-inf')\n",
    "            maxA = ''\n",
    "                        \n",
    "            for a in grid.all_actions():\n",
    "                \n",
    "                # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if not targets: continue\n",
    "                \n",
    "                v = 0\n",
    "                for next_state, prob in targets.items():\n",
    "                    v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "                \n",
    "                if debug: print (f\"      {a} -> {v}\")\n",
    "                if v >= maxV:\n",
    "                    maxV = v\n",
    "                    maxA = a\n",
    "\n",
    "            if maxA != A[s]:\n",
    "                A[s] = maxA\n",
    "                policy_changed = True\n",
    "            \n",
    "        if not policy_changed:\n",
    "            break\n",
    "            \n",
    "    # last value evaluation\n",
    "    V = iterative_policy_evaluation(grid, pi, 0.9)\n",
    "\n",
    "    return A, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | +0.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -10.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.50 +0.00 | +0.00 +0.00 | +0.00 +0.50 | +0.50 +0.00 |\n",
      "|    +0.50    |    +0.50    |    +0.00    |    +0.50    |    +0.50    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.33 +0.33 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.33    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.00 +0.00 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.00    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.33    |    +0.25    |    +0.25    |    +0.25    |    +0.33    |\n",
      "| +0.00 +0.33 | +0.25 +0.25 | +0.25 +0.25 | +0.25 +0.25 | +0.33 +0.00 |\n",
      "|    +0.33    |    +0.25    |    +0.25    |    +0.25    |    +0.33    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.50    |    +0.33    |    +0.33    |    +0.33    |    +0.00    |\n",
      "| +0.00 +0.50 | +0.33 +0.33 | +0.33 +0.33 | +0.33 +0.33 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +4.26 | +4.85 | +0.00 | +6.22 | +7.02 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +4.85 | +5.50 | +6.22 | +7.02 | +7.91 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.50 | +6.22 | +0.00 | +7.91 | +8.90 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +6.22 | +7.02 | +7.91 | +8.90 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +7.02 | +7.91 | +8.90 | +10.00 | +0.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_negative_grid(step_cost=-0.1):\n",
    "    g = build_standard_grid()\n",
    "    g.rewards.update({ s:step_cost for s in g.actions.keys() })\n",
    "    g.rewards.update({ (0,0):2*step_cost })\n",
    "    return g\n",
    "    \n",
    "neg_grid = build_negative_grid(-0.1)\n",
    "\n",
    "A,V = policy_iteration(neg_grid, 0.9)\n",
    "\n",
    "print_value (neg_grid.rewards, neg_grid)\n",
    "print_policy(random_policy, neg_grid)\n",
    "print_value (V, neg_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | +0.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -10.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +1.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +0.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +0.00    |    +1.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +0.56 | +0.89 | +0.00 | +2.55 | +3.05 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +3.67 | +3.70 | +4.70 | +6.63 | +7.27 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +4.31 | +4.10 | +0.00 | +7.54 | +8.42 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.11 | +6.09 | +7.22 | +8.50 | +9.73 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.71 | +6.79 | +8.05 | +9.51 | +0.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "wind_grid = build_windy_grid(-0.1)\n",
    "\n",
    "A,V = policy_iteration(wind_grid, 0.5)\n",
    "\n",
    "print_value (wind_grid.rewards, wind_grid)\n",
    "print_policy(fixed_policy, wind_grid)\n",
    "print_value (V, wind_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | +0.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -10.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "Policy\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +1.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +1.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +1.00    |    +0.00    |    +1.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +1.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +0.00    |    +0.00    |    +1.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +1.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +1.00    |    +1.00    |    +1.00    |    +1.00    |    +1.00    |\n",
      "| +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "| +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +1.00 | +0.00 +0.00 |\n",
      "|    +0.00    |    +0.00    |    +0.00    |    +0.00    |    +0.00    |\n",
      "+-------------+-------------+-------------+-------------+-------------+\n",
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +0.56 | +0.89 | +0.00 | +2.55 | +3.05 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +3.67 | +3.70 | +4.70 | +6.63 | +7.27 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +4.31 | +4.10 | +0.00 | +7.54 | +8.42 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.11 | +6.09 | +7.22 | +8.50 | +9.73 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| +5.71 | +6.79 | +8.05 | +9.51 | +0.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    }
   ],
   "source": [
    "def build_windy_grid(step_cost=-0.1):\n",
    "    g = build_negative_grid(step_cost)\n",
    "    \n",
    "    def transitions(s, a):\n",
    "        \n",
    "        if s not in g.actions: return {}\n",
    "        if a not in g.actions[s]: return {}\n",
    "        \n",
    "        n = len(g.actions[s]) - 1\n",
    "        \n",
    "        go_u = ( min(g.height-1,s[0]+1), s[1]                )\n",
    "        go_d = ( max(0, s[0]-1)        , s[1]                )\n",
    "        go_r = ( s[0]                  , min(g.width-1,s[1]+1) )\n",
    "        go_l = ( s[0]                  , max(0, s[1]-1)      )\n",
    "        \n",
    "        if a=='U': return {go_u: 0.8 , go_d:0, go_r:0.2, go_l:0 }\n",
    "        if a=='D': return {go_u: 0, go_d: 0.8 , go_r:0, go_l:0.2 }\n",
    "        if a=='R': return {go_u: 0, go_d:0.2, go_r: 0.8, go_l:0 }\n",
    "        if a=='L': return {go_u: 0.2, go_d:0, go_r:0, go_l: 0.8  }\n",
    "\n",
    "    g.transitions = transitions\n",
    "    return g\n",
    "\n",
    "wind_grid = build_windy_grid()\n",
    "\n",
    "A,V = policy_iteration(wind_grid, 0.9, debug=False)\n",
    "\n",
    "print_value (wind_grid.rewards, wind_grid)\n",
    "print_policy(build_fixed_policy(A), wind_grid)\n",
    "print_value (V, wind_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(grid, gamma, debug=False):\n",
    "    \n",
    "    # initialize the value function to 0\n",
    "    V = { s:0.0 for s in grid.all_states() }\n",
    "    \n",
    "    \n",
    "    # loop until stabilization\n",
    "    loop = 0\n",
    "    while True:\n",
    "        if debug: print (f\"--- Loop #{loop}\")\n",
    "        loop += 1\n",
    "        \n",
    "        delta = 0\n",
    "        \n",
    "        # enumerate all states\n",
    "        for s in grid.all_states():\n",
    "            if debug: print (f\"   State {s}\")\n",
    "            \n",
    "            # if terminal, the value is still zero\n",
    "            if grid.is_terminal_state(s):\n",
    "                if debug: print (f\"   -> terminal\")\n",
    "                continue\n",
    "                \n",
    "                \n",
    "            maxV = float('-inf')\n",
    "            for a in grid.all_actions():\n",
    "                \n",
    "                # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "                targets = grid.transitions(s,a)\n",
    "                if not targets: continue\n",
    "                \n",
    "                v = 0\n",
    "                for next_state, prob in targets.items():\n",
    "                    v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "                \n",
    "                if debug: print (f\"      {a} -> {v}\")\n",
    "                if v >= maxV:\n",
    "                    maxV = v\n",
    "            \n",
    "            delta = max(delta, abs(maxV - V[s]))\n",
    "            if debug: print (f\"   Vs={V[s]} -> {maxV} : delta={delta}\")\n",
    "                    \n",
    "            V[s] = maxV\n",
    "            \n",
    "        if (delta < 1e-3):\n",
    "            break\n",
    "    \n",
    "    for s in grid.all_states():\n",
    "        if debug: print (f\"    State {s}\")\n",
    "\n",
    "        maxV = float('-inf')\n",
    "        maxA = ''\n",
    "\n",
    "        for a in grid.all_actions():\n",
    "\n",
    "            # get p(s′,r|s,a), the distribution of possible targets if we take action 'a' from state 's'\n",
    "            targets = grid.transitions(s,a)\n",
    "            if not targets: continue\n",
    "\n",
    "            v = 0\n",
    "            for next_state, prob in targets.items():\n",
    "                v += prob * (grid.rewards.get(next_state, 0.0) + gamma * V[next_state])\n",
    "\n",
    "            if debug: print (f\"      {a} -> {v}\")\n",
    "            if v >= maxV:\n",
    "                maxV = v\n",
    "                maxA = a\n",
    "\n",
    "        A[s] = maxA\n",
    "\n",
    "    return A, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.20 | -0.10 | +0.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -10.00 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | -0.10 |\n",
      "+-------+-------+-------+-------+-------+\n",
      "| -0.10 | -0.10 | -0.10 | -0.10 | +10.00 |\n",
      "+-------+-------+-------+-------+-------+\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'build_windy_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-c61d8d8f3e94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint_value\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwind_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwind_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mprint_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_windy_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint_value\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'build_windy_policy' is not defined"
     ]
    }
   ],
   "source": [
    "A,V = value_iteration(wind_grid, 0.9, debug=False)\n",
    "\n",
    "print_value (wind_grid.rewards, wind_grid)\n",
    "print_policy(build_windy_policy(A), std_grid)\n",
    "print_value (V, std_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def first_visit_mc_predition(grid, pi, N, gamma):\n",
    "\n",
    "    states_and_returns = {} # key:state - value:array of returns\n",
    "    V = {}\n",
    "    \n",
    "    for i in range(N):\n",
    "        snr = play_episode(grid, pi, gamma)\n",
    "        seen_states = set()\n",
    "        for (s,g) in snr:\n",
    "            if s not in seen_states:\n",
    "                seen_states.add(s)\n",
    "                if s in states_and_returns:\n",
    "                    states_and_returns[s].append(g)\n",
    "                else:\n",
    "                    states_and_returns[s] = [g]\n",
    "        \n",
    "        for s, returns in states_and_returns.items():\n",
    "            V[s] = np.mean(returns)\n",
    "            \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an action to do from state s according to the policy pi\n",
    "def select_action(grid, pi, s):\n",
    "    A = grid.actions[s]\n",
    "    p = [pi(grid, a, s) for a in A]\n",
    "    return np.random.choice(A, p=p)\n",
    "    \n",
    "\n",
    "def play_episode(grid, pi, gamma):\n",
    "    grid.restart(random=True)\n",
    "    \n",
    "    # play until gme is over\n",
    "    s = grid.state \n",
    "    states_and_rewards = [(s,0)]\n",
    "    while not grid.is_game_over():    \n",
    "        a = select_action(grid, pi, s)\n",
    "        print(s,\"->\", a)\n",
    "        r = grid.move(a)\n",
    "        s = grid.state\n",
    "        states_and_rewards.append((s,r))\n",
    "        \n",
    "    # iteratively compute returns from rewards\n",
    "    G = 0\n",
    "    states_and_returns = []\n",
    "    for s,r in reversed(states_and_rewards):\n",
    "        states_and_returns.append((s,G))\n",
    "        G = r + gamma * G\n",
    "\n",
    "    states_and_returns.reverse()\n",
    "\n",
    "    return states_and_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1) -> R\n",
      "{(4, 1): 0, (2, 1): 0.2, (3, 2): 0.8, (3, 0): 0}\n",
      "(3, 2) -> R\n",
      "{(4, 2): 0, (2, 2): 0.2, (3, 3): 0.8, (3, 1): 0}\n",
      "(3, 3) -> R\n",
      "{(4, 3): 0, (2, 3): 0.2, (3, 4): 0.8, (3, 2): 0}\n",
      "(3, 4) -> D\n",
      "{(4, 4): 0, (2, 4): 0.8, (3, 4): 0, (3, 3): 0.2}\n",
      "(3, 3) -> R\n",
      "{(4, 3): 0, (2, 3): 0.2, (3, 4): 0.8, (3, 2): 0}\n",
      "(3, 4) -> D\n",
      "{(4, 4): 0, (2, 4): 0.8, (3, 4): 0, (3, 3): 0.2}\n",
      "(2, 4) -> D\n",
      "{(3, 4): 0, (1, 4): 0.8, (2, 4): 0, (2, 3): 0.2}\n",
      "(1, 4) -> D\n",
      "{(2, 4): 0, (0, 4): 0.8, (1, 4): 0, (1, 3): 0.2}\n",
      "(0, 4) -> D\n",
      "{(1, 4): 0, (0, 4): 0, (0, 3): 0.2}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-b077d3574122>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_visit_mc_predition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwind_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint_policy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfixed_policy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwind_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint_value\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-f46c3ab7fd39>\u001b[0m in \u001b[0;36mfirst_visit_mc_predition\u001b[1;34m(grid, pi, N, gamma)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0msnr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplay_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mseen_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msnr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-dc35a327b60f>\u001b[0m in \u001b[0;36mplay_episode\u001b[1;34m(grid, pi, gamma)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"->\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mstates_and_rewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-6ae617bb52c6>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mV\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "\n",
    "V = first_visit_mc_predition(wind_grid, fixed_policy, 100, 0.9)\n",
    "\n",
    "print_policy(fixed_policy, wind_grid)\n",
    "print_value (V, std_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
